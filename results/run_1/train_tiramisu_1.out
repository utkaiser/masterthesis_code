/work/09080/lkaiser/maverick2/run_1/bash_scripts
c228-105.maverick2.tacc.utexas.edu
Thu Oct  6 14:53:33 CDT 2022
start training tiramisu 256
gpu available: True | n of gpus: 2
setting up data
total number of data points: 10000
2022-10-06 14:59:41.074350 epoch 1: loss: 0.00727 | coarse loss: 0.00386
/home1/09080/lkaiser/.local/lib/python3.6/site-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode)
/home1/09080/lkaiser/.local/lib/python3.6/site-packages/torch/nn/functional.py:3509: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
2022-10-06 15:04:48.509930 epoch 2: loss: 0.00230 | coarse loss: 0.00386
2022-10-06 15:09:55.874380 epoch 3: loss: 0.00211 | coarse loss: 0.00386
2022-10-06 15:15:03.766450 epoch 4: loss: 0.00197 | coarse loss: 0.00386
2022-10-06 15:20:11.034351 epoch 5: loss: 0.00187 | coarse loss: 0.00386
2022-10-06 15:25:19.187029 epoch 6: loss: 0.00184 | coarse loss: 0.00386
2022-10-06 15:30:26.881418 epoch 7: loss: 0.00181 | coarse loss: 0.00386
2022-10-06 15:35:34.701599 epoch 8: loss: 0.00172 | coarse loss: 0.00386
2022-10-06 15:40:43.360438 epoch 9: loss: 0.00170 | coarse loss: 0.00386
2022-10-06 15:45:51.046900 epoch 10: loss: 0.00165 | coarse loss: 0.00386
2022-10-06 15:50:58.698233 epoch 11: loss: 0.00162 | coarse loss: 0.00386
2022-10-06 15:56:07.078808 epoch 12: loss: 0.00160 | coarse loss: 0.00386
2022-10-06 16:01:14.694451 epoch 13: loss: 0.00156 | coarse loss: 0.00386
2022-10-06 16:06:22.557095 epoch 14: loss: 0.00154 | coarse loss: 0.00386
2022-10-06 16:11:30.568990 epoch 15: loss: 0.00152 | coarse loss: 0.00386
2022-10-06 16:16:39.366702 epoch 16: loss: 0.00149 | coarse loss: 0.00386
2022-10-06 16:21:48.746064 epoch 17: loss: 0.00149 | coarse loss: 0.00386
2022-10-06 16:26:56.533042 epoch 18: loss: 0.00146 | coarse loss: 0.00386
2022-10-06 16:32:04.602064 epoch 19: loss: 0.00146 | coarse loss: 0.00386
2022-10-06 16:37:12.344566 epoch 20: loss: 0.00144 | coarse loss: 0.00386
2022-10-06 16:42:20.171734 epoch 21: loss: 0.00144 | coarse loss: 0.00386
2022-10-06 16:47:28.101755 epoch 22: loss: 0.00142 | coarse loss: 0.00386
2022-10-06 16:52:35.956679 epoch 23: loss: 0.00141 | coarse loss: 0.00386
2022-10-06 16:57:43.866416 epoch 24: loss: 0.00141 | coarse loss: 0.00386
2022-10-06 17:02:51.630686 epoch 25: loss: 0.00140 | coarse loss: 0.00386
2022-10-06 17:07:59.792391 epoch 26: loss: 0.00139 | coarse loss: 0.00386
2022-10-06 17:13:07.323293 epoch 27: loss: 0.00139 | coarse loss: 0.00386
2022-10-06 17:18:14.803371 epoch 28: loss: 0.00138 | coarse loss: 0.00386
2022-10-06 17:23:22.564250 epoch 29: loss: 0.00137 | coarse loss: 0.00386
2022-10-06 17:28:30.959506 epoch 30: loss: 0.00137 | coarse loss: 0.00386
2022-10-06 17:33:38.681605 epoch 31: loss: 0.00136 | coarse loss: 0.00386
2022-10-06 17:38:47.429975 epoch 32: loss: 0.00136 | coarse loss: 0.00386
2022-10-06 17:43:55.281250 epoch 33: loss: 0.00135 | coarse loss: 0.00386
2022-10-06 17:49:03.612534 epoch 34: loss: 0.00135 | coarse loss: 0.00386
2022-10-06 17:54:11.132801 epoch 35: loss: 0.00135 | coarse loss: 0.00386
2022-10-06 17:59:18.793534 epoch 36: loss: 0.00135 | coarse loss: 0.00386
2022-10-06 18:04:27.513709 epoch 37: loss: 0.00134 | coarse loss: 0.00386
2022-10-06 18:09:35.133569 epoch 38: loss: 0.00134 | coarse loss: 0.00386
2022-10-06 18:14:42.948658 epoch 39: loss: 0.00133 | coarse loss: 0.00386
2022-10-06 18:19:50.629733 epoch 40: loss: 0.00133 | coarse loss: 0.00386
2022-10-06 18:24:58.842431 epoch 41: loss: 0.00133 | coarse loss: 0.00386
2022-10-06 18:30:07.006037 epoch 42: loss: 0.00132 | coarse loss: 0.00386
2022-10-06 18:35:14.721691 epoch 43: loss: 0.00132 | coarse loss: 0.00386
2022-10-06 18:40:22.618209 epoch 44: loss: 0.00133 | coarse loss: 0.00386
2022-10-06 18:45:30.372469 epoch 45: loss: 0.00132 | coarse loss: 0.00386
2022-10-06 18:50:38.703744 epoch 46: loss: 0.00132 | coarse loss: 0.00386
2022-10-06 18:55:46.288627 epoch 47: loss: 0.00131 | coarse loss: 0.00386
2022-10-06 19:00:54.385342 epoch 48: loss: 0.00131 | coarse loss: 0.00386
2022-10-06 19:06:02.065638 epoch 49: loss: 0.00131 | coarse loss: 0.00386
2022-10-06 19:11:09.867290 epoch 50: loss: 0.00130 | coarse loss: 0.00386
2022-10-06 19:16:17.767303 epoch 51: loss: 0.00130 | coarse loss: 0.00386
2022-10-06 19:21:25.570775 epoch 52: loss: 0.00131 | coarse loss: 0.00386
2022-10-06 19:26:33.207807 epoch 53: loss: 0.00130 | coarse loss: 0.00386
2022-10-06 19:31:40.932642 epoch 54: loss: 0.00131 | coarse loss: 0.00386
2022-10-06 19:36:47.701317 epoch 55: loss: 0.00130 | coarse loss: 0.00386
2022-10-06 19:41:54.450599 epoch 56: loss: 0.00130 | coarse loss: 0.00386
2022-10-06 19:47:01.865569 epoch 57: loss: 0.00129 | coarse loss: 0.00386
2022-10-06 19:52:09.557012 epoch 58: loss: 0.00130 | coarse loss: 0.00386
2022-10-06 19:57:16.588168 epoch 59: loss: 0.00129 | coarse loss: 0.00386
2022-10-06 20:02:24.712668 epoch 60: loss: 0.00129 | coarse loss: 0.00386
2022-10-06 20:07:32.619881 epoch 61: loss: 0.00129 | coarse loss: 0.00386
2022-10-06 20:12:41.045366 epoch 62: loss: 0.00129 | coarse loss: 0.00386
2022-10-06 20:17:47.684890 epoch 63: loss: 0.00129 | coarse loss: 0.00386
2022-10-06 20:22:54.971517 epoch 64: loss: 0.00128 | coarse loss: 0.00386
2022-10-06 20:28:03.707481 epoch 65: loss: 0.00129 | coarse loss: 0.00386
2022-10-06 20:33:12.531270 epoch 66: loss: 0.00128 | coarse loss: 0.00386
Traceback (most recent call last):
  File "../models/train_multi_gpu.py", line 95, in <module>
    train(model_name = model_name, resolution = model_resolution, fine_coarse_scale=int(scaler))
  File "../models/train_multi_gpu.py", line 56, in train
    for i, data in enumerate(train_loader):
  File "/home1/09080/lkaiser/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home1/09080/lkaiser/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1186, in _next_data
    idx, data = self._get_data()
  File "/home1/09080/lkaiser/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 1152, in _get_data
    success, data = self._try_get_data()
  File "/home1/09080/lkaiser/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 990, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/usr/lib64/python3.6/multiprocessing/queues.py", line 113, in get
    return _ForkingPickler.loads(res)
  File "/home1/09080/lkaiser/.local/lib/python3.6/site-packages/torch/multiprocessing/reductions.py", line 289, in rebuild_storage_fd
    fd = df.detach()
  File "/usr/lib64/python3.6/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/usr/lib64/python3.6/multiprocessing/resource_sharer.py", line 87, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/usr/lib64/python3.6/multiprocessing/connection.py", line 487, in Client
    c = SocketClient(address)
  File "/usr/lib64/python3.6/multiprocessing/connection.py", line 614, in SocketClient
    s.connect(address)
FileNotFoundError: [Errno 2] No such file or directory
