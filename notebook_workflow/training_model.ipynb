{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This notebook explains how to train the end-to-end model of the master's thesis “Wave propagation aided by Deep Learning” by Luis Kaiser, supervised by Prof. Tsai (University of Texas Austin) and Prof. Klingenberg (University of Wuerzburg), in practice. More information about the algorithm can be found in my [code](https://github.com/utkaiser/masterthesis_code) or [writeup](https://github.com/utkaiser/masterthesis_writing)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, make sure that you have install all necessary libraries specified in `requirements.txt` using `pip` or `pip3` depending on your setup by running the command below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip3 install --upgrade pip\n",
    "!pip3 install -r requirements.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Parts of this code are extracted from the files `models/train_end_to_end.py`. We make this notebook easier to read with less complex dependencies in the code. We first load the data, then set up the model with optimizer and lastly train and evaluate the model. We use `datagen_test.npz` as our train and test data. Please note that the model does not train well for small datasets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from models.model_end_to_end import Model_end_to_end\n",
    "import torch\n",
    "from models.model_utils import get_params, fetch_data_end_to_end, save_model\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train_model(\n",
    "        model_name = \"test\",\n",
    "        lr = .001,\n",
    "        batch_size = 1,\n",
    "        n_epochs = 2,\n",
    "        downsampling_model = \"Interpolation\",\n",
    "        upsampling_model = \"UNet3\",\n",
    "        res_scaler = 2,\n",
    "        model_res = 128,\n",
    "        data_paths = \"datagen_test.npz\",\n",
    "        val_paths = \"datagen_test2.npz\"\n",
    "):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : (string) name of model, used as name for output-file containing model parameters\n",
    "    lr : (float) learning rate of model\n",
    "    batch_size : (int) batch size\n",
    "    n_epochs : (int) number of iterations model sees all data; i.e. amount of epochs model is trained\n",
    "    downsampling_model : (string) name of downsampling model\n",
    "    upsampling_model : (string) name of upsampling model\n",
    "    res_scaler : (int) scale the model downsamples the input to speed up coarse solver computations\n",
    "    model_res : (int) resolution of model\n",
    "    data_paths : (string)\n",
    "    val_paths : (string)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    trained model parameters in \".pt\"-file\n",
    "    '''\n",
    "\n",
    "    # model setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    param_dict = get_params(\"0\")  # get dict of params contains all specifications necessary\n",
    "    model = Model_end_to_end(param_dict, downsampling_model, upsampling_model, res_scaler, model_res).double()\n",
    "    model = torch.nn.DataParallel(model).to(device)  # multi-GPU use\n",
    "\n",
    "    # data setup\n",
    "    train_loader, val_loader = fetch_data_end_to_end([data_paths], batch_size, [val_paths])\n",
    "\n",
    "    # deep learning setup\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)  # initialize optimizer\n",
    "    loss_f = torch.nn.MSELoss()  # initialize loss function\n",
    "\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # training\n",
    "        model.train()\n",
    "\n",
    "        train_loss_list = []  # initialize list to store loss values of training\n",
    "\n",
    "        for i, data in enumerate(train_loader):  # iterate over datapoints in train_loader\n",
    "            loss_list = []  # create tmp loss list for backpropagating multiple losses at once\n",
    "            n_snaps = data[0].shape[1]  # number of snapshots defined by data input\n",
    "            data = data[0].to(device)  # use GPUs if available for faster training\n",
    "\n",
    "            # choose n_snaps start indices, possible points are in [0, ..., n_snaps - 2] and chosen randomly\n",
    "            # i.e. (input_idx * dt_star) point in time is when we start using the data\n",
    "            for input_idx in random.choices(range(n_snaps - 2), k=n_snaps):\n",
    "                input_tensor = data[:, input_idx].detach()  # detach because if not computation graph would go too far back\n",
    "\n",
    "                # one-step loss function (this loop is used to show that we can use something else for\n",
    "                # \"input_idx + 2\" to get a multi-step-loss approach (cf. paper)\n",
    "                for label_idx in range(input_idx + 1, input_idx + 2):\n",
    "                    label = data[:, label_idx, :3]\n",
    "                    output = model(input_tensor)  # apply end-to-end model\n",
    "                    loss_list.append(loss_f(output, label))  # save loss\n",
    "\n",
    "                    # IMPORTANT: save current result to use for next iteration if multi-step-loss used\n",
    "                    input_tensor = torch.cat((output, input_tensor[:, 3].unsqueeze(dim=1)), dim=1)\n",
    "\n",
    "            # optimizer stepping\n",
    "            optimizer.zero_grad()\n",
    "            sum(loss_list).backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # save loss to later print out\n",
    "            train_loss_list.append(np.array([l.cpu().detach().numpy() for l in loss_list]).mean())\n",
    "\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # initialize list to save losses\n",
    "            val_loss_list = []\n",
    "\n",
    "            for i, data in enumerate(val_loader):  # iterate over datapoints in val_loader\n",
    "                n_snaps = data[0].shape[1]  # number of snapshots defined by data input\n",
    "                data = data[0].to(device)  # use GPUs if available for faster training\n",
    "\n",
    "                input_tensor = data[:, 0].detach()  # detach because if not computation graph would go too far back\n",
    "                vel = input_tensor[:, 3].unsqueeze(dim=1)  # access velocity profile in input_tensor\n",
    "\n",
    "                for label_idx in range(1, n_snaps):  # advance a wave field for (n_snaps - 1) time steps\n",
    "                    label = data[:, label_idx, :3]\n",
    "                    output = model(input_tensor)  # apply end-to-end model\n",
    "\n",
    "                    # get and save loss (this could be optimized by using a metric)\n",
    "                    val_loss_list.append(loss_f(output, label).item())\n",
    "\n",
    "                    # IMPORTANT: save current result to use for next iteration if multi-step-loss used\n",
    "                    input_tensor = torch.cat((output, vel), dim=1)\n",
    "\n",
    "\n",
    "        print(f'epoch %d, train loss: %.5f, test loss: %.5f'\n",
    "              %(epoch + 1, np.array(train_loss_list).mean(), np.array(val_loss_list).mean()))\n",
    "\n",
    "    # save model parameters as a \".pt\"-file, use empty string for directory path\n",
    "    save_model(model, model_name, \"\")\n",
    "\n",
    "\n",
    "train_model()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
